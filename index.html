<!doctype html><html lang=en><head><meta name=generator content="Hugo 0.121.1"><title>Zekun's Zone</title>
<meta name=description content="Portfolio and personal blog of Zekun Li from the University of Minnesota."><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.650f674fc19266f4ea6c7b54bb6a4ffad10dfa641e3bea6de0a8472a99807590.css integrity="sha256-ZQ9nT8GSZvTqbHtUu2pP+tEN+mQeO+pt4KhHKpmAdZA="><link rel=icon type=image/png href=/images/site/zk-favicon_hube1e813c2c2088aca28be8f9b74a2dca_16753_42x0_resize_box_3.png><meta property="og:title" content="Zekun's Zone"><meta property="og:type" content="website"><meta property="og:description" content="Portfolio and personal blog of Zekun Li, PhD student from the University of Minnesota"><meta property="og:image" content="/images/author/john.png"><meta property="og:url" content="https://hugo-toha.github.io"></head><body data-spy=scroll data-target=#top-navbar data-offset=100><nav class="navbar navbar-expand-xl top-navbar initial-navbar" id=top-navbar><div class=container><a class=navbar-brand href=/><img src=/images/site/zk-inverted-transp_hu109326400271da0b02bd55eeaf589ffc_103282_42x0_resize_box_3.png id=logo alt=Logo>
Zekun's Zone</a>
<button class="navbar-toggler navbar-dark" id=navbar-toggler type=button data-toggle=collapse data-target=#top-nav-items aria-label=menu>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=#home>Home</a></li><li class=nav-item><a class=nav-link href=#about>About</a></li><li class=nav-item><a class=nav-link href=#experiences>Experiences</a></li><li class=nav-item><a class=nav-link href=#publications>Publications</a></li><li class=nav-item><a class=nav-link href=#projects>Selected Projects</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=#accomplishments>Accomplishments</a>
<a class=dropdown-item href=#recent-posts>Recent Posts</a></div></li><div class=dropdown-divider id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts/>Posts</a></li><li class=nav-item><a class=nav-link href=https://toha-guides.netlify.app/posts/>Docs</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/zk-main-transp_huafc6864d78c8fdec171d529e6199f4b7_107023_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/zk-inverted-transp_hu109326400271da0b02bd55eeaf589ffc_103282_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><div class="container-fluid home" id=home><style>#homePageBackgroundImageDivStyled{background-image:url(/images/site/background5_hu72bc6652bce5d3f3b73b29c75238c806_324616_500x0_resize_box_3.png)}@media(min-width:500px) and (max-width:800px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background5_hu72bc6652bce5d3f3b73b29c75238c806_324616_800x0_resize_box_3.png)}}@media(min-width:801px) and (max-width:1200px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background5_hu72bc6652bce5d3f3b73b29c75238c806_324616_1200x0_resize_box_3.png)}}@media(min-width:1201px) and (max-width:1500px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background5_hu72bc6652bce5d3f3b73b29c75238c806_324616_1500x0_resize_box_3.png)}}@media(min-width:1501px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background5.png)}}</style><span class=on-the-fly-behavior></span><div id=homePageBackgroundImageDivStyled class="background container-fluid"></div><div class="container content text-center"><img src=/images/author/zekun_hub35c5a96dbe046beec5ae87b3c28a938_421003_148x148_fit_box_3.png class="rounded-circle mx-auto d-block img-fluid" alt="Author Image"><h1 class=greeting>Hi, I am Zekun</h1><div class=typing-carousel><span id=ityped class=ityped></span>
<span class=ityped-cursor></span></div><ul id=typing-carousel-data><li>I am a CS PhD Student</li><li>I am a Computer Vision practioner</li><li>I love historical maps</li></ul><a href=#about class=arrow-center aria-label="Read More - Zekun"><i class="arrow bounce fa fa-chevron-down"></i></a></div></div><div class="container-fluid section-holder d-flex bg-white"><div class="container anchor p-lg-5 about-section" id=about><div class="row pt-sm-2 pt-md-4 align-self-center"><div class=col-sm-6><h3 class=p-1>Zekun Li</h3><h5 class=p-1>Ph.D. Student
at <a href=https://twin-cities.umn.edu/ title="University of Minnesota, Twin Cities" target=_blank rel=noopener>University of Minnesota, Twin Cities</a></h5><p class="p-1 text-justify">I am a computer science Ph.D. student at the University of Minnesota (UMN), working with Professor Yao-Yi Chiang. My research interest is on the geospatial data analysis with <strong>computer vision</strong> and <strong>natural language processing</strong> techniques. I have worked on text detection of historical map labels, connecting separated text labels, linking recognized place names to existing knowledge bases (entity linking) and label type inference (entity typing). Previously at the University of Southern California (USC), I also worked on object detection projects in the Information Sciences Institute (ISI) and Spatial Sciences Institute (SSI).</p><div class="text-container ml-auto"><ul class="social-link d-flex"><li><a href=https://github.com/zekun-li/ title=Github target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://stackoverflow.com/users/7242982/zekun title=Stackoverflow target=_blank rel=noopener><i class="fab fa-stack-overflow"></i></a></li><li><a href=https://www.linkedin.com/in/lizekun/ title=LinkedIn target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://www.facebook.com/profile.php?id=100006970454676" title=Facebook target=_blank rel=noopener><i class="fab fa-facebook"></i></a></li></ul></div><a href=/files/Zekun_Resume_202311.pdf title="My resume" target=#><button class="btn btn-dark">My resume</button></a></div><div class="col-sm-6 pt-5 pl-md-4 pl-sm-3 pt-sm-0"><div class=row><div class="col-6 col-lg-4 p-2"><div><a href=https://pytorch.org/ target=_blank rel="noopener noreferrer"><img src=https://i.imgur.com/WZUj67T.png alt=pytorch></a></div></div><div class="col-6 col-lg-4 p-2"><div><a href=https://keras.io/ target=_blank rel="noopener noreferrer"><img src=https://i.imgur.com/dVxI1WF.png alt=keras></a></div></div><div class="col-6 col-lg-4 p-2"><div><a href=https://www.docker.com/ target=_blank rel="noopener noreferrer"><img src=https://i.imgur.com/zetD3Rx.png alt=docker></a></div></div><div class="col-6 col-lg-4 p-2"><div><a href=https://ubuntu.com/ target=_blank rel="noopener noreferrer"><img src=https://i.imgur.com/6F0C4dL.png alt=ubuntu></a></div></div><div class="col-6 col-lg-4 p-2"><div><a href=https://www.centos.org/ target=_blank rel="noopener noreferrer"><img src=https://i.imgur.com/vL9dpSA.png alt=centos></a></div></div><div class="col-6 col-lg-4 p-2"><div><a href=https://www.raspberrypi.org/ target=_blank rel="noopener noreferrer"><img src=https://i.imgur.com/0cvKlf1.png alt=raspberrypi></a></div></div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-dimmed"><div class="container-fluid anchor pb-5 experiences-section"><h1 class=text-center><span id=experiences></span>Experiences</h1><div class="container timeline text-justify"><div class="row align-items-center d-flex"><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">1</div></div><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Text Detection and Recognition for Historical Maps</h5><h6><a href=https://spatial.usc.edu/ title="Spatial Sciences Institute, USC" target=_blank rel=noopener>Spatial Sciences Institute, USC</a></h6><p class=text-muted>Dec 2019 - Aug 2020,
Los Angeles</p></div><p></p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Built a deep neural network for detecting text of various font size, style and orientation angles on historical map patches. The network is able to handle text regions of arbitrary shapes</li><li>Designed the network to highlight probable text regions and then predict accurate bounding boxes given both map features and text probability distributions in a coarse-to-fine manner</li></ul></div></div><div class="row horizontal-line"><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div><div class="col-10 col-lg-8"><hr></div><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div></div><div class="row align-items-center justify-content-end d-flex"><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Generating Historical Maps from online Maps</h5><h6><a href=https://spatial.usc.edu/ title="Spatial Sciences Institute, USC" target=_blank rel=noopener>Spatial Sciences Institute, USC</a></h6><p class=text-muted>Aug 2018 - Dec 2019,
Los Angeles</p></div><p></p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Synthesized historical maps from Open Street Map tiles with conditional generative adversarial networks</li><li>The network generated background and foreground separately using different targets to solve the content mismatch problem in online maps and historical maps</li><li>Used the synthesized historical maps as the base-map and automatically place text labels on them to provide a large amount of training data for text detection networks</li></ul></div><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">2</div></div></div><div class="row horizontal-line"><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div><div class="col-10 col-lg-8"><hr></div><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div></div><div class="row align-items-center d-flex"><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">3</div></div><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Synthetic Face Generation for Facial Landmark Detection</h5><h6><a href=https://www.amazon.com/ title=Amazon target=_blank rel=noopener>Amazon</a></h6><p class=text-muted>May 2020 - Aug 2020,
Seattle (Remote)</p></div><p></p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Built a pipeline to generate synthetic face images with landmark annotations using 3D modeling application Makehuman and rendering application Blender</li><li>Rendered the images from 3D models with various poses, camera setting, lighting conditions and backgrounds</li><li>Verified that the 2D landmark detection task and the 3D mesh prediction task can both benefit from the large amount of generated synthetic images</li></ul></div></div><div class="row horizontal-line"><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div><div class="col-10 col-lg-8"><hr></div><div class="col-1 col-lg-2 timeline-side-div"><div class=corner></div></div></div><div class="row align-items-center justify-content-end d-flex"><div class="col-10 col-lg-8"><div class=experience-entry-heading><h5>Automated Visual Data Extraction from Chart Images</h5><h6><a href=https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/ title="Microsoft Research Asia" target=_blank rel=noopener>Microsoft Research Asia</a></h6><p class=text-muted>May 2019 - Aug 2019,
Beijing, China</p></div><p></p><h6 class=text-muted>Responsibilities:</h6><ul class=justify-content-around><li>Built a pipeline to automatically infer numerical values for each chart given the column chart images</li><li>Applied trident-net to extract the chart object heights. Designed a ruler encoding module to interpret the y-axis information to convert the objects from pixel-space to ruler space to generate reading</li><li>The ruler encoding module focuses on the minimum and maximum values of the ruler to decide the numerical range that the charts represent</li></ul></div><div class="col-1 col-lg-2 text-center vertical-line d-inline-flex justify-content-center"><div class="circle font-weight-bold">4</div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-white"><div class="container-fluid anchor pb-5 publications-section" id=publications><h1 class=text-center><span id=publications></span>Publications</h1><div class="container ml-auto text-center"><div class="btn-group flex-wrap" role=pub-group id=publication-filter-buttons><button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-all>
All
</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-cv>
Computer Vision
</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-nlp>
Natural Language Processing
</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-maps>
Maps and Geospaital Data</button></div></div><div class="container filtr-publications"><div class=row id=publication-card-holder><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-nlp,pub-maps><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">SpaBERT Pretrained Language Models on Geographic Data for Geo-Entity Representation</h5><div class=sub-title><span><a href=https://2022.emnlp.org/>EMNLP 2022</a></span>
<span class=ml-2></span></div><div class=authors><span class=mr-2><a href=https://zekun-li.github.io>Zekun Li</a></span>
<span class=mr-2><a href=https://jina-kim.github.io/>Jina Kim</a></span>
<span class=mr-2><a href=https://yaoyichi.github.io/>Yao-Yi Chiang</a></span>
<span class=mr-2><a href=https://muhaochen.github.io/>Muhao Chen</a></span></div></div><div class=card-body><p>The paper proposes a novel spatial language model called SpaBERT that captures the spatial context of named geographic entities (geo-entities) in geospatial data. The model is based on the hypothesis that the characteristics of a geo-entity can be inferred by its surrounding entities, similar to word meanings in linguistic context.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">language model
</span><span class="btn badge btn-info ml-1 p-2">spatial data</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://aclanthology.org/2022.findings-emnlp.200 target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-cv><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">ACE: Anchor-free Corner Evolution for Real-time Arbitrarily-oriented Object Detection</h5><div class=sub-title><span><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Transaction on Image Processing (TIP) 2022</a></span>
<span class=ml-2></span></div><div class=authors><span class=mr-2><a href=https://example.com>Pengwen Dai</a></span>
<span class=mr-2><a href=https://example.com>Siyuan Yao</a></span>
<span class=mr-2><a href=https://zekun-li.github.io>Zekun Li</a></span>
<span class=mr-2><a href=https://example.com>Sanyi Zhang</a></span>
<span class=mr-2><a href=https://example.com>Xiaochun Cao</a></span></div></div><div class=card-body><p>The paper proposes a novel model for detecting arbitrarily-oriented objects, such as texts/hands or objects in aerial images. The model evolves the axis-aligned bounding box to an oriented quadrilateral box using contour information.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">object detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://ieeexplore.ieee.org/abstract/document/9761381 target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-cv><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">ChartOCR: Data Extraction from Charts Images via a Deep Hybrid Framework.</h5><div class=sub-title><span><a href=https://wacv2021.thecvf.com/home>WACV 2021</a></span>
<span class=ml-2></span></div><div class=authors><span class=mr-2><a href=https://example.com>Junyu Luo</a></span>
<span class=mr-2><a href=https://zekun-li.github.io>Zekun Li</a></span>
<span class=mr-2><a href=https://example.com>Jinpeng Wang</a></span>
<span class=mr-2><a href=https://example.com>Chin-Yew Lin</a></span></div></div><div class=card-body><p>The paper proposes an unified method, called ChartOCR, to extract data from various types of chart images, including bar charts, line charts, and pie charts. We combine deep learning and rule-based methods to achieve generalization ability and obtain accurate and semantic-rich intermediate results.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">object detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://openaccess.thecvf.com/content/WACV2021/papers/Luo_ChartOCR_Data_Extraction_From_Charts_Images_via_a_Deep_Hybrid_WACV_2021_paper.pdf target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-cv,pub-maps><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">An Automatic Approach for Generating Rich, Linked Geo-Metadata from Historical Map Images</h5><div class=sub-title><span><a href=https://www.kdd.org/kdd2020/>KDD 2020</a></span>
<span class=ml-2></span></div><div class=authors><span class=mr-2><a href=https://zekun-li.github.io>Zekun Li</a></span>
<span class=mr-2><a href=https://example.com>Yao-Yi Chiang</a></span>
<span class=mr-2><a href=https://example.com>Sasan Tavakkol</a></span>
<span class=mr-2><a href=https://example.com>Basel Shbita</a></span>
<span class=mr-2><a href=https://example.com>Johannes H. Uhl</a></span>
<span class=mr-2><a href=https://example.com>Stefan Leyk</a></span>
<span class=mr-2><a href=https://example.com>Craig A. Knoblock</a></span></div></div><div class=card-body><p>We present an end-to-end approach that automatically processes historical map images to extract their text content and generate a set of metadata linked to large geographic databases. The approach combines OCR with geocoding to accurately identify location phrases and assign geospatial coordinates.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">maps
</span><span class="btn badge btn-info ml-1 p-2">object detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://dl.acm.org/doi/pdf/10.1145/3394486.3403381 target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-cv,pub-maps><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Synthetic Map Generation to Provide Unlimited Training Data for Historical Map Text Detection</h5><div class=sub-title><span><a href=https://wacv2021.thecvf.com/home>SIGSPATIAL GeoAI workshop 2021</a></span>
<span class=ml-2></span></div><div class=authors><span class=mr-2><a href=https://zekun-li.github.io>Zekun Li</a></span>
<span class=mr-2><a href=https://example.com>Runyu Guan</a></span>
<span class=mr-2><a href=https://example.com>Qianmu Yu</a></span>
<span class=mr-2><a href=https://example.com>Yao-Yi Chiang</a></span>
<span class=mr-2><a href=https://example.com>Craig A. Knoblock</a></span></div></div><div class=card-body><p>Many text detection algorithms have been proposed to locate text regions in map images automatically, but most of the algorithms are trained on out-of-domain datasets. This paper introduces a method to automatically generate an unlimited amount of annotated historical map images for training text detection models.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">maps
</span><span class="btn badge btn-info ml-1 p-2">text detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=files/GEOAI_2021.pdf target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-maps><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Combining Remote-Sensing-Derived Data and Historical Maps for Long-Term Back-Casting of Urban Extents.</h5><div class=sub-title><span><a href=https://www.mdpi.com/2072-4292/13>Remote Sensing 2021</a></span>
<span class=ml-2></span></div><div class=authors><span class=mr-2><a href=https://example.com>Johannes H. Uhl</a></span>
<span class=mr-2><a href=https://example.com>Stefan Leyk</a></span>
<span class=mr-2><a href=https://zekun-li.github.io>Zekun Li</a></span>
<span class=mr-2><a href=https://example.com>Weiwei Duan</a></span>
<span class=mr-2><a href=https://example.com>Basel Shbita</a></span>
<span class=mr-2><a href=https://example.com>Yao-Yi Chiang</a></span>
<span class=mr-2><a href=https://example.com>Craig A. Knoblock</a></span></div></div><div class=card-body><p>The paper proposes a framework that uses remote sensing data (the Global Human Settlement Layer, GHSL) and georeferenced historical maps to generate historical urban extents for the early 20th century.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">object detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://www.mdpi.com/2072-4292/13/18/3672 target=_blank rel=noopener role=button>Details</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-cv><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Weighted Feature Pooling Network in Template-Based Recognition</h5><div class=sub-title><span><a href=https://researchr.org/conference/accv-2019>ACCV 2019</a></span>
<span class=ml-2></span></div><div class=authors><span class=mr-2><a href=https://zekun-li.github.io>Zekun Li</a></span>
<span class=mr-2><a href=https://example.com>Yue Wu</a></span>
<span class=mr-2><a href=https://example.com>Wael Abd-Almageed</a></span>
<span class=mr-2><a href=https://example.com>Prem Natarajan</a></span></div></div><div class=card-body><p>The paper proposes a template-based learning approach for computer vision tasks, where multiple instances of a concept are available. The method dynamically predicts weights that consider noise and redundancy to aggregate image-level features into a single template-level representation.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">object detection</span></div><div class=details-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=files/accv_0585.pdf target=_blank rel=noopener role=button>Details</a></div></div></div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-dimmed"><div class="container-fluid anchor pb-5 projects-section" id=projects><h1 class=text-center><span id=projects></span>Selected Projects</h1><div class="container ml-auto text-center"><div class="btn-group flex-wrap" role=group id=project-filter-buttons></div></div><div class="container filtr-projects"><div class=row id=project-card-holder><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, map processing,deep learning'><div class="card mt-1"><div class=card><a class=card-header href=https://knowledge-computing.github.io/mapkurator-doc/ target=_blank rel=noopener><div><div class=d-flex><img class=card-img-xs src=/images/sections/projects/github_hu8fe07ce4a4840486246ec69e395a42f5_8640_24x24_fit_box_3.png alt="mapKurator system"><h5 class="card-title mb-0">mapKurator system</h5></div><div class=sub-title><span>Team Lead</span>
<span>March 2020 - Present</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>A deep learning tool to process scanned historical maps. Performs text detection & recoginition, postOCR correction and entity linking.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">map processing
</span><span class="badge btn-info">deep learning</span></div><div class=project-btn-holder><span><a class="btn btn-outline-info btn-sm" href=https://knowledge-computing.github.io/mapkurator-doc/ target=#>Details</a></span></div></div></div></div></div></div><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, nlp,geospatial'><div class="card mt-1"><div class=card><a class=card-header href=https://github.com/zekun-li/spabert target=_blank rel=noopener><div><div class=d-flex><img class=card-img-xs src=/images/sections/projects/github_hu8fe07ce4a4840486246ec69e395a42f5_8640_24x24_fit_box_3.png alt=SpaBERT><h5 class="card-title mb-0">SpaBERT</h5></div><div class=sub-title><span>First Author</span>
<span>Jan 2021 - Nov 2022</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>SpaBERT extends BERT to capture linearized spatial context, while incorporating a spatial coordinate embedding mechanism to preserve spatial relations of entities in the 2-dimensional space.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">nlp
</span><span class="badge btn-info">geospatial</span></div><div class=project-btn-holder><span><a class="btn btn-outline-info btn-sm" href=https://github.com/zekun-li/spabert target=#>Details</a></span></div></div></div></div></div></div><div class="col-sm-12 col-md-6 col-lg-4 p-2 filtr-item" data-category='all, historical maps'><div class="card mt-1"><div class=card><a class=card-header href=https://github.com/zekun-li/generate_synthetic_historical_maps target=_blank rel=noopener><div><div class=d-flex><img class=card-img-xs src=/images/sections/projects/github_hu8fe07ce4a4840486246ec69e395a42f5_8640_24x24_fit_box_3.png alt="Synthetic Map Generation"><h5 class="card-title mb-0">Synthetic Map Generation</h5></div><div class=sub-title><span>First Author</span>
<span>Jun 2019 - Aug 2020</span></div></div></a><div class="card-body text-justify pt-1 pb-1"><p>Utilized cycle-GAN to convert open street map (OSM) images into historical map style.</p><div class=project-card-footer><div class=project-tags-holder><span class="badge btn-info">historical maps</span></div><div class=project-btn-holder><span><a class="btn btn-outline-info btn-sm" href=https://github.com/zekun-li/generate_synthetic_historical_maps target=#>Details</a></span></div></div></div></div></div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-white"><div class="container-fluid anchor pb-5 accomplishments-section"><h1 class=text-center><span id=accomplishments></span>Accomplishments</h1><div class=container><div class=row id=acomplishment-card-holder><div class="col-md-12 col-lg-6 p-2"><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Ordnance Survey Award</h5><div class=sub-title><span><a href=https://www.cartography.org.uk/ title="British Cartographic Society" target=_blank rel=noopener>British Cartographic Society</a></span>
<span class=ml-2>September 2022</span></div></div><div class=card-body><p><strong>Mishmash: A Mix of Old and New</strong> won the Ordnance Survey Award 2022. I adopted cycleGAN to generate synthetic historical maps from Open Street Map (OSM) vector data.</p></div><div class=card-footer><a class="btn btn-outline-info ml-1 pl-2 mb-2" href="https://youtu.be/J7xRyDLue6U?t=72" target=_blank rel=noopener role=button>View Certificate</a></div></div></div><div class="col-md-12 col-lg-6 p-2"><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">AI for Critical Mineral Assessment Competition</h5><div class=sub-title><span><a href=https://www.darpa.mil/ title=DARPA target=_blank rel=noopener>DARPA</a></span>
<span class=ml-2>December 2022</span></div></div><div class=card-body><p>The Map Feature Extraction Challenge required us to identify and label map features–lines, polygons, and points– that appear in the legend of historical maps. Our team <strong>isi-umn</strong> won the <strong>first place</strong>.</p></div><div class=card-footer><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=https://criticalminerals.darpa.mil/Results target=_blank rel=noopener role=button>View Certificate</a></div></div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-dimmed"><div class="container-fluid anchor pb-5 recent-posts-section"><h1 class=text-center><span id=recent-posts></span>Recent Posts</h1><div class=container><div class=row id=recent-post-cards><div class="col-lg-4 col-md-6 pt-2 post-card"><a href=/posts/github-keys/ title="Two Github Accounts with Two SSH Keys" class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/github-keys/hero.png alt="Card image cap"></div><div class=card-body><h5 class=card-title>Two Github Accounts with Two SSH Keys</h5><p class="card-text post-summary">To use two different GitHub accounts, one for work and one for personal use, you will need to create two separate RSA keys.
Create Two RSA keys Here are the steps to create two RSA keys:
Open your terminal.
Input the following command, replacing &ldquo;email@example.com&rdquo; with your email address:
ssh-keygen -t rsa -C "[email@example.com](mailto:email@example.com)"
When asked to enter a file in which to save the key, type in a unique name for each key.</p></div><div class=card-footer><span class=float-left>March 27, 2023</span>
<a href=/posts/github-keys/ title=Read class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class="col-lg-4 col-md-6 pt-2 post-card"><a href=/posts/chatgpt-translation/ title="ChatGPT for Novel Translation" class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/chatgpt-translation/hero.png alt="Card image cap"></div><div class=card-body><h5 class=card-title>ChatGPT for Novel Translation</h5><p class="card-text post-summary">ChatGPT is a powerful and versatile large language model that can assist in many NLP tasks. One such task is translation, and in this blog post, we will show you how to use ChatGPT to translate Chinese web novels into English.
Step 1: Create an OpenAI account Before you can use the ChatGPT model for novel translation, you need to create an OpenAI account to obtain an API key. This key will grant you access to the platform&rsquo;s powerful tools and resources, including the ChatGPT model.</p></div><div class=card-footer><span class=float-left>March 25, 2023</span>
<a href=/posts/chatgpt-translation/ title=Read class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class="col-lg-4 col-md-6 pt-2 post-card"><a href=/posts/pi-timelapse/ title="Raspberry Pi Timelapse Video" class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/pi-timelapse/hero.png alt="Card image cap"></div><div class=card-body><h5 class=card-title>Raspberry Pi Timelapse Video</h5><p class="card-text post-summary">Recording a timelapse video of a sunset can be a fantastic way to capture the beauty of nature in a condensed form. Without a bulky professional camera, you can still record a timelapse video with Raspberry Pi easily. In this blog post, we&rsquo;ll introduce the raspistill and ffmepg command for creating videos.
We can take a look at a demo video shot with pi-camera v1.3 on Raspberry Pi. It was a maganificant sunset in Minnesota.</p></div><div class=card-footer><span class=float-left>November 14, 2022</span>
<a href=/posts/pi-timelapse/ title=Read class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div></div></div></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://zekun-li.github.io#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://zekun-li.github.io#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://zekun-li.github.io#publications>Publications</a></li><li class=nav-item><a class=smooth-scroll href=https://zekun-li.github.io#projects>Selected Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://zekun-li.github.io#accomplishments>Accomplishments</a></li><li class=nav-item><a class=smooth-scroll href=https://zekun-li.github.io#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:li002666%20[Shift+2]%20umn.edu target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>li002666 [Shift+2] umn.edu</span></a></li><li><a href=https://github.com/zekun-li target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>zekun-li</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2023 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.e4017f8c61913f573edc5e53b54f6a0ae943ace9de57d7800db57e47c916732a.js integrity="sha256-5AF/jGGRP1c+3F5TtU9qCulDrOneV9eADbV+R8kWcyo=" defer></script></body></html>